{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779792e0-33e8-4d7d-ba5a-c620bd248475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97254\\Desktop\\NLP_FINAL_PROJECT_DREAMS\\dreams_interpreter\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0136ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "True\n",
      "Quadro RTX 3000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is working\n",
    "print(torch.cuda.get_device_name(0))  # Check your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20baec02-671c-4add-bfc6-0b131c0d27ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dream Symbol</th>\n",
       "      <th>Interpretation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aardvark</td>\n",
       "      <td>To see an aardvark in your dream indicates tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abandonment</td>\n",
       "      <td>To dream that you are abandoned suggests that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abduction</td>\n",
       "      <td>To dream of being abducted indicates that you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aborigine</td>\n",
       "      <td>To see an Aborigine in your dream represents b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abortion</td>\n",
       "      <td>To dream that you have an abortion suggests th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dream Symbol                                     Interpretation\n",
       "0     Aardvark  To see an aardvark in your dream indicates tha...\n",
       "1  Abandonment  To dream that you are abandoned suggests that ...\n",
       "2    Abduction  To dream of being abducted indicates that you ...\n",
       "3    Aborigine  To see an Aborigine in your dream represents b...\n",
       "4     Abortion  To dream that you have an abortion suggests th..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare the dataset\n",
    "df = pd.read_csv('dreams_interpretations.csv')\n",
    "data = df[['Dream Symbol', 'Interpretation']].dropna()\n",
    "train_data, val_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718df98c-8555-4c32-8b4c-3ae361ef9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        input_text = f\"Interpret the dream: {row['Dream Symbol']}\"\n",
    "        target_text = row['Interpretation']\n",
    "\n",
    "        # Tokenize input and label\n",
    "        encoding = self.tokenizer(\n",
    "            input_text, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': target_encoding['input_ids'].squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "076b18ad-fd03-430e-baa6-5f921d321368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MLM\\Desktop\\Niv\\studies\\AFEKA\\NLP\\final project NLP\\LORA model\\lora_model\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MLM\\.cache\\huggingface\\hub\\models--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2846b9a-11e0-410e-8e12-6fd05df4213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Split dataset\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = DreamDataset(train_data, tokenizer)\n",
    "val_dataset = DreamDataset(val_data, tokenizer)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# Set device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Label smoothing loss (helps with overfitting)\n",
    "def compute_loss(logits, labels):\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(label_smoothing=0.1, ignore_index=tokenizer.pad_token_id)\n",
    "    return loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30acc702-da49-495c-bf2a-b686be0f59c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 181/181 [30:30<00:00, 10.12s/it, loss=3.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished, avg loss: 4.2320302955353455\n",
      "Validation Loss: 3.6747285231300024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 181/181 [52:58<00:00, 17.56s/it, loss=3.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished, avg loss: 3.7265347683627303\n",
      "Validation Loss: 3.5500590645748638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 181/181 [52:39<00:00, 17.46s/it, loss=2.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished, avg loss: 3.571800096258933\n",
      "Validation Loss: 3.4803445235542627\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = compute_loss(outputs.logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished, avg loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = compute_loss(outputs.logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "487fa17d-d4c9-4c62-b9ef-483bbe5a051e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dream_interpreter_t5\\\\tokenizer_config.json',\n",
       " 'dream_interpreter_t5\\\\special_tokens_map.json',\n",
       " 'dream_interpreter_t5\\\\spiece.model',\n",
       " 'dream_interpreter_t5\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"dream_interpreter_t5\")\n",
    "tokenizer.save_pretrained(\"dream_interpreter_t5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Evaluation function\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Turn off gradient tracking\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\", ncols=100):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Get predictions (take argmax for sequence generation tasks)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # You can also compute metrics like BLEU score or others\n",
    "    return avg_val_loss, all_predictions, all_labels\n",
    "\n",
    "\n",
    "\n",
    "# %% Evaluate on validation set\n",
    "avg_val_loss, all_predictions, all_labels = evaluate_model(model, val_loader, device)\n",
    "\n",
    "# %% Example of comparing predictions and labels\n",
    "print(f\"Example predictions: {all_predictions[:5]}\")\n",
    "print(f\"Example true labels: {all_labels[:5]}\")\n",
    "\n",
    "# Optionally, calculate other metrics like BLEU score, accuracy, etc.\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Calculate BLEU score (use this if your task involves sequence generation)\n",
    "def calculate_bleu_score(predictions, labels):\n",
    "    # Convert predictions and labels to a list of lists (for BLEU scoring)\n",
    "    ref = [[label.split()] for label in labels]  # Convert true labels to list of words\n",
    "    pred = [prediction.split() for prediction in predictions]  # Convert predictions to list of words\n",
    "\n",
    "    return corpus_bleu(ref, pred)\n",
    "\n",
    "# Example usage of BLEU score calculation\n",
    "bleu_score = calculate_bleu_score(all_predictions, all_labels)\n",
    "print(f\"BLEU score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cbf5ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dream: I was flying high above the clouds\n",
      "Interpretation: To dream that you were flying high above the clouds symbolizes your desire to be in control of your life.\n",
      "==================================================\n",
      "Dream: I lost all my teeth\n",
      "Interpretation: To dream that you have lost all your teeth represents a loss of self-esteem and self esteem.\n",
      "==================================================\n",
      "Dream: I was being chased by a lion\n",
      "Interpretation: To see or be chased by a lion in your dream represents your desire to be in control of your life.\n",
      "==================================================\n",
      "Dream: I was underwater in a deep ocean\n",
      "Interpretation: To dream that I was underwater in a deep ocean indicates that you need to be more aware of your surroundings.\n",
      "==================================================\n",
      "Dream: I found a hidden treasure chest\n",
      "Interpretation: To dream that you have found a hidden treasure chest suggests that there is something you need to find.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# %% Generate Dream Interpretation for Sample Inputs\n",
    "def generate_interpretation(model, tokenizer, device, dream_input, max_length=128):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    # Tokenize the input dream\n",
    "    input_text = f\"Dream: {dream_input}\\nInterpretation:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "\n",
    "    # Generate the interpretation (output text)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=256,  # Maximum length of the generated interpretation\n",
    "            num_beams=4,  # Beam search for more diverse outputs\n",
    "            no_repeat_ngram_size=2,  # Prevent repetition\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode the generated ids back to text\n",
    "    interpretation = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return interpretation\n",
    "\n",
    "# %% Example input dreams\n",
    "sample_dreams = [\n",
    "    \"I was flying high above the clouds\",\n",
    "    \"I lost all my teeth\",\n",
    "    \"I was being chased by a lion\",\n",
    "    \"I was underwater in a deep ocean\",\n",
    "    \"I found a hidden treasure chest\"\n",
    "]\n",
    "\n",
    "# %% Generate and display interpretations for sample dreams\n",
    "for dream in sample_dreams:\n",
    "    interpretation = generate_interpretation(model, tokenizer, device, dream)\n",
    "    print(f\"Dream: {dream}\")\n",
    "    print(f\"Interpretation: {interpretation}\")\n",
    "    print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_interpreter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
