{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T19:38:08.556206Z",
     "start_time": "2025-02-05T19:37:25.473763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load the uploaded CSV file (Dream symbols and interpretations)\n",
    "file_path = 'dreams_interpretations.csv'\n",
    "dream_data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract relevant columns for model training\n",
    "dream_data[\"text\"] = \"Dream: \" + dream_data[\"Dream Symbol\"] + \" Interpretation: \" + dream_data[\"Interpretation\"]\n",
    "\n",
    "# Keep only the text column\n",
    "dream_data_prepared = dream_data[[\"text\"]]\n",
    "\n",
    "# Check for missing values\n",
    "dream_data_prepared = dream_data_prepared.dropna()\n",
    "\n",
    "# Split the data into train (80%) and validation (20%) sets\n",
    "train_df, val_df = train_test_split(dream_data_prepared, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply the tokenizer to the train and validation datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Load the pre-trained DistilBERT model for masked language modeling\n",
    "model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Define a data collator for MLM (randomly masks words)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # 15% of tokens will be masked\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_finetuned_dreams\",  # Directory to save the fine-tuned model\n",
    "    num_train_epochs=3,                          # Number of epochs\n",
    "    per_device_train_batch_size=8,               # Training batch size\n",
    "    per_device_eval_batch_size=8,                # Evaluation batch size\n",
    "    save_strategy=\"epoch\",                       # Save at the end of each epoch\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    logging_steps=10,                            # Log every 10 steps\n",
    "    logging_dir=\"./logs\",                        # Directory for logs\n",
    "    save_total_limit=2,                          # Keep only the latest 2 checkpoints\n",
    "    load_best_model_at_end=True,                 # Load best model at the end\n",
    "    report_to=\"tensorboard\"                      # Log to TensorBoard\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "trainer.save_model(\"./distilbert_finetuned_dreams\")\n",
    "tokenizer.save_pretrained(\"./distilbert_finetuned_dreams\")\n"
   ],
   "id": "9c37d7a88429da58",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/721 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37f61133b53f4597999f604140f47127"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/181 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6e352d30d60466599e779a199618806"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baraky\\PycharmProjects\\NLP Project\\.myenv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baraky\\AppData\\Local\\Temp\\ipykernel_1520\\3365980275.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='273' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/273 00:22 < 20:21, 0.22 it/s, Epoch 0.07/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 72\u001B[0m\n\u001B[0;32m     62\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     63\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     64\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     68\u001B[0m     data_collator\u001B[38;5;241m=\u001B[39mdata_collator,\n\u001B[0;32m     69\u001B[0m )\n\u001B[0;32m     71\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 72\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# Save the fine-tuned model and tokenizer\u001B[39;00m\n\u001B[0;32m     75\u001B[0m trainer\u001B[38;5;241m.\u001B[39msave_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./distilbert_finetuned_dreams\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP Project\\.myenv\\Lib\\site-packages\\transformers\\trainer.py:2171\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2169\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2170\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2172\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2176\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP Project\\.myenv\\Lib\\site-packages\\transformers\\trainer.py:2531\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2524\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2525\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m   2526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2527\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[0;32m   2528\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[0;32m   2529\u001B[0m )\n\u001B[0;32m   2530\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m-> 2531\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2533\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2534\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2535\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2536\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2537\u001B[0m ):\n\u001B[0;32m   2538\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2539\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP Project\\.myenv\\Lib\\site-packages\\transformers\\trainer.py:3712\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   3709\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_accepts_loss_kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_func \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3710\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n\u001B[1;32m-> 3712\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3714\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP Project\\.myenv\\Lib\\site-packages\\accelerate\\accelerator.py:1964\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[1;34m(self, loss, **kwargs)\u001B[0m\n\u001B[0;32m   1962\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler\u001B[38;5;241m.\u001B[39mscale(loss)\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1963\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1964\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP Project\\.myenv\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    625\u001B[0m     )\n\u001B[1;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP Project\\.myenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP Project\\.myenv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T19:38:19.017119Z",
     "start_time": "2025-02-05T19:38:16.917627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned DistilBERT model for masked word prediction\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"./distilbert_finetuned_dreams\", tokenizer=\"./distilbert_finetuned_dreams\")\n",
    "\n",
    "# ðŸ”¹ Retrieve the original text dataset before tokenization\n",
    "train_texts = train_df[\"text\"].tolist()  # Get original train dreams\n",
    "val_texts = val_df[\"text\"].tolist()  # Get original validation (test) dreams\n",
    "\n",
    "# ðŸ”¹ Select 2 random dreams from the validation dataset (test set)\n",
    "sample_test_dreams = random.sample(val_texts, 2)\n",
    "\n",
    "# ðŸ”¹ Select 2 random dreams from the training dataset (train set)\n",
    "sample_train_dreams = random.sample(train_texts, 2)\n",
    "\n",
    "# Combine both lists\n",
    "sample_dreams = sample_test_dreams + sample_train_dreams\n",
    "\n",
    "# Function to predict missing words in a dream interpretation\n",
    "def predict_interpretation(dream_text):\n",
    "    \"\"\"\n",
    "    Uses the fine-tuned DistilBERT model to predict missing words in a dream interpretation.\n",
    "    \"\"\"\n",
    "    masked_text = f\"Dream: {dream_text} Interpretation: This dream represents [MASK] emotions related to [MASK] and subconscious [MASK].\"\n",
    "\n",
    "    # Generate predictions for each masked token\n",
    "    predictions = mask_filler(masked_text)\n",
    "\n",
    "    # Construct interpretation with predicted words\n",
    "    filled_text = masked_text\n",
    "    for prediction in predictions:\n",
    "        filled_text = filled_text.replace(\"[MASK]\", prediction[0]['token_str'], 1)\n",
    "\n",
    "    return filled_text\n",
    "\n",
    "# Process and print results for all selected dreams\n",
    "for i, dream in enumerate(sample_dreams):\n",
    "    interpretation = predict_interpretation(dream)\n",
    "    dataset_type = \"Test\" if i < 2 else \"Train\"  # First 2 from Test, Last 2 from Train\n",
    "    print(f\"({dataset_type} Set) Dream: {dream}\")\n",
    "    print(f\"Predicted Interpretation: {interpretation}\")\n",
    "    print(\"-\" * 50)\n"
   ],
   "id": "6a9e810bb0a7439",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test Set) Dream: Dream: Yourself Interpretation: To see yourself in your dream is a reflection of how you act and behave in your waking life.Â Consider what you are doing and how you are feeling in the dream for additional significance.\n",
      "Predicted Interpretation: Dream: Dream: Yourself Interpretation: To see yourself in your dream is a reflection of how you act and behave in your waking life.Â Consider what you are doing and how you are feeling in the dream for additional significance. Interpretation: This dream represents your emotions related to life and subconscious desires.\n",
      "--------------------------------------------------\n",
      "(Test Set) Dream: Dream: Screen Interpretation: To see a window screen in your dream implies that you are being cautiously optimistic about the good news you receive.\n",
      "Predicted Interpretation: Dream: Dream: Screen Interpretation: To see a window screen in your dream implies that you are being cautiously optimistic about the good news you receive. Interpretation: This dream represents your emotions related to dreams and subconscious desires.\n",
      "--------------------------------------------------\n",
      "(Train Set) Dream: Dream: Lamp Interpretation: To see a lamp in your dream symbolizes guidance, hope, inspiration, enlightenment and reassurance. If the lamp is dimly lit or unlit, then it suggests that you are feeling overwhelmed by emotional issues. You have lost your ability to find your own way or see things clearly.\n",
      "Predicted Interpretation: Dream: Dream: Lamp Interpretation: To see a lamp in your dream symbolizes guidance, hope, inspiration, enlightenment and reassurance. If the lamp is dimly lit or unlit, then it suggests that you are feeling overwhelmed by emotional issues. You have lost your ability to find your own way or see things clearly. Interpretation: This dream represents your emotions related to dreams and subconscious desires.\n",
      "--------------------------------------------------\n",
      "(Train Set) Dream: Dream: Fetus Interpretation: To see a fetus in your dream symbolizes a newly developing relationship or idea in your waking life. Something creative is happening. Alternatively, you may be expressing difficulties in some situation or relationship.\n",
      "Predicted Interpretation: Dream: Dream: Fetus Interpretation: To see a fetus in your dream symbolizes a newly developing relationship or idea in your waking life. Something creative is happening. Alternatively, you may be expressing difficulties in some situation or relationship. Interpretation: This dream represents your emotions related to sex and subconscious desires.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T19:55:19.231826Z",
     "start_time": "2025-02-02T19:55:12.727018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned DistilBERT model for masked word prediction\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"./distilbert_finetuned_dreams\", tokenizer=\"./distilbert_finetuned_dreams\")\n",
    "\n",
    "# ðŸ”¹ Retrieve the original text dataset before tokenization\n",
    "train_texts = train_df[\"text\"].tolist()  # Get original train dreams\n",
    "val_texts = val_df[\"text\"].tolist()  # Get original validation (test) dreams\n",
    "\n",
    "# ðŸ”¹ Select 2 random dreams from the validation dataset (test set)\n",
    "sample_test_dreams = random.sample(val_texts, 2)\n",
    "\n",
    "# ðŸ”¹ Select 2 random dreams from the training dataset (train set)\n",
    "sample_train_dreams = random.sample(train_texts, 2)\n",
    "\n",
    "# Combine both lists\n",
    "sample_dreams = sample_test_dreams + sample_train_dreams\n",
    "\n",
    "# ðŸ”¹ Function to predict missing words in a dream interpretation\n",
    "def predict_interpretation(dream_text):\n",
    "    \"\"\"\n",
    "    Uses the fine-tuned DistilBERT model to predict missing words in a dream interpretation.\n",
    "    \"\"\"\n",
    "    masked_text = f\"Dream: {dream_text}. Freud would say this dream symbolizes [MASK], possibly connected to [MASK] feelings of [MASK].\"\n",
    "\n",
    "\n",
    "    # Generate predictions for masked words (without top_p or temperature)\n",
    "    predictions = mask_filler(masked_text)\n",
    "\n",
    "    # Construct interpretation with predicted words\n",
    "    filled_text = masked_text\n",
    "    for prediction in predictions:\n",
    "        filled_text = filled_text.replace(\"[MASK]\", prediction[0]['token_str'], 1)\n",
    "\n",
    "    return filled_text\n",
    "\n",
    "# Process and print results for all selected dreams\n",
    "for i, dream in enumerate(sample_dreams):\n",
    "    interpretation = predict_interpretation(dream)\n",
    "    dataset_type = \"Test\" if i < 2 else \"Train\"  # First 2 from Test, Last 2 from Train\n",
    "    print(f\"({dataset_type} Set) Dream: {dream}\")\n",
    "    print(f\"Predicted Interpretation: {interpretation}\")\n",
    "    print(\"-\" * 50)\n"
   ],
   "id": "e6746821b963d594",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test Set) Dream: Dream: Dartboard Interpretation: To see a dartboard in your dream indicates that you are feeling hostility from someone. You need to express your anger and feelings more directly. Alternatively, the dartboard may symbolize a goal that you are aiming for. You need to try and take a shot at something new and overcome your fear of failure.\n",
      "Predicted Interpretation: Dream: Dream: Dartboard Interpretation: To see a dartboard in your dream indicates that you are feeling hostility from someone. You need to express your anger and feelings more directly. Alternatively, the dartboard may symbolize a goal that you are aiming for. You need to try and take a shot at something new and overcome your fear of failure.. Freud would say this dream symbolizes anger, possibly connected to your feelings of anger.\n",
      "--------------------------------------------------\n",
      "(Test Set) Dream: Dream: ater\n",
      "Tots Interpretation: To see or eat tater tots in your dream implies that you are focusing too much on petty matters.\n",
      "Predicted Interpretation: Dream: Dream: ater\n",
      "Tots Interpretation: To see or eat tater tots in your dream implies that you are focusing too much on petty matters.. Freud would say this dream symbolizes anger, possibly connected to your feelings of guilt.\n",
      "--------------------------------------------------\n",
      "(Train Set) Dream: Dream: Safety\n",
      "Pin Interpretation: To see a safety pin in your dream indicates that a situation is on the verge of falling apart causing anxiety or fear. You feel that the livelihood of the relationship depends on you.\n",
      "Predicted Interpretation: Dream: Dream: Safety\n",
      "Pin Interpretation: To see a safety pin in your dream indicates that a situation is on the verge of falling apart causing anxiety or fear. You feel that the livelihood of the relationship depends on you.. Freud would say this dream symbolizes anger, possibly connected to your feelings of guilt.\n",
      "--------------------------------------------------\n",
      "(Train Set) Dream: Dream: Bamboo Interpretation: To see bamboo in your dream symbolizes trustworthiness, strength and resilience. You are able to easily bounce back from setbacks and disappointments in your life. Alternatively, it refers to strong ties/bonds and fair dealings.\n",
      "Predicted Interpretation: Dream: Dream: Bamboo Interpretation: To see bamboo in your dream symbolizes trustworthiness, strength and resilience. You are able to easily bounce back from setbacks and disappointments in your life. Alternatively, it refers to strong ties/bonds and fair dealings.. Freud would say this dream symbolizes connection, possibly connected to your feelings of guilt.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T20:13:40.042972Z",
     "start_time": "2025-02-05T20:13:37.647800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned DistilBERT model for masked word prediction\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"./distilbert_finetuned_dreams\", tokenizer=\"./distilbert_finetuned_dreams\")\n",
    "\n",
    "# ðŸ”¹ Retrieve the original text dataset before tokenization\n",
    "train_texts = train_df[\"text\"].tolist()  # Get original train dreams\n",
    "val_texts = val_df[\"text\"].tolist()  # Get original validation (test) dreams\n",
    "\n",
    "# ðŸ”¹ Select 2 random dreams from the validation dataset (test set)\n",
    "sample_test_dreams = random.sample(val_texts, 2)\n",
    "\n",
    "# ðŸ”¹ Select 2 random dreams from the training dataset (train set)\n",
    "sample_train_dreams = random.sample(train_texts, 2)\n",
    "\n",
    "samples_own=[\n",
    "    \"I was flying high above the clouds\",\n",
    "    \"I lost all my teeth\",\n",
    "    \"I was being chased by a lion\",\n",
    "    \"I was underwater in a deep ocean\",\n",
    "    \"I found a hidden treasure chest\"\n",
    "    \"I see baby open pandora box\"\n",
    "]\n",
    "\n",
    "# Combine both lists\n",
    "sample_dreams = sample_test_dreams + sample_train_dreams + samples_own\n",
    "\n",
    "# ðŸ”¹ Function to predict missing words in a dream interpretation\n",
    "def predict_interpretation(dream_text):\n",
    "    \"\"\"\n",
    "    Uses the fine-tuned DistilBERT model to predict missing words in a dream interpretation.\n",
    "    \"\"\"\n",
    "    masked_text = f\"Dream: {dream_text}. Freud would say this dream symbolizes [MASK], possibly connected to [MASK] feelings of [MASK].\"\n",
    "\n",
    "\n",
    "    # Generate predictions for masked words (without top_p or temperature)\n",
    "    predictions = mask_filler(masked_text)\n",
    "\n",
    "    # Construct interpretation with predicted words\n",
    "    filled_text = masked_text\n",
    "    for prediction in predictions:\n",
    "        filled_text = filled_text.replace(\"[MASK]\", prediction[0]['token_str'], 1)\n",
    "\n",
    "    return filled_text\n",
    "\n",
    "# Process and print results for all selected dreams\n",
    "for i, dream in enumerate(sample_dreams):\n",
    "    interpretation = predict_interpretation(dream)\n",
    "    dataset_type = \"Test\" if i < 2 else \"Train\"  # First 2 from Test, Last 2 from Train\n",
    "    print(f\"({dataset_type} Set) Dream: {dream}\")\n",
    "    print(f\"Predicted Interpretation: {interpretation}\")\n",
    "    print(\"-\" * 50)\n"
   ],
   "id": "1dd342e9fd758691",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test Set) Dream: Dream: Babysitter Interpretation: To dream that you are babysitting suggests that you need to care for the child within yourself.\n",
      "Predicted Interpretation: Dream: Dream: Babysitter Interpretation: To dream that you are babysitting suggests that you need to care for the child within yourself.. Freud would say this dream symbolizes sadness, possibly connected to your feelings of abandonment.\n",
      "--------------------------------------------------\n",
      "(Test Set) Dream: Dream: Elf Interpretation: To see an elf in your dream refers to some imbalance and disharmony in your life. The elf often serves as a guide of the soul. Alternatively, it suggests that you need to be more carefree, worry-free, and light-hearted.\n",
      "Predicted Interpretation: Dream: Dream: Elf Interpretation: To see an elf in your dream refers to some imbalance and disharmony in your life. The elf often serves as a guide of the soul. Alternatively, it suggests that you need to be more carefree, worry-free, and light-hearted.. Freud would say this dream symbolizes anger, possibly connected to your feelings of guilt.\n",
      "--------------------------------------------------\n",
      "(Train Set) Dream: Dream: Ki Interpretation: To see a killer in your dream suggests that an essential aspect of your emotions have been cut off. You feel that you are losing your identity and your individuality. Alternatively, this dream may represent purification and the healing process. You are standing up for yourself and putting a dramatic end to something.\n",
      "Predicted Interpretation: Dream: Dream: Ki Interpretation: To see a killer in your dream suggests that an essential aspect of your emotions have been cut off. You feel that you are losing your identity and your individuality. Alternatively, this dream may represent purification and the healing process. You are standing up for yourself and putting a dramatic end to something.. Freud would say this dream symbolizes anger, possibly connected to your feelings of guilt.\n",
      "--------------------------------------------------\n",
      "(Train Set) Dream: Dream: Urination Interpretation: To see urine in your dream represents the feelings you have rejected. Alternatively, the dream may be a pun on your \"pissy\" attitude.\n",
      "Predicted Interpretation: Dream: Dream: Urination Interpretation: To see urine in your dream represents the feelings you have rejected. Alternatively, the dream may be a pun on your \"pissy\" attitude.. Freud would say this dream symbolizes anger, possibly connected to your feelings of guilt.\n",
      "--------------------------------------------------\n",
      "(Train Set) Dream: I was flying high above the clouds\n",
      "Predicted Interpretation: Dream: I was flying high above the clouds. Freud would say this dream symbolizes sadness, possibly connected to your feelings of loss.\n",
      "--------------------------------------------------\n",
      "(Train Set) Dream: I lost all my teeth\n",
      "Predicted Interpretation: Dream: I lost all my teeth. Freud would say this dream symbolizes anger, possibly connected to your feelings of loss.\n",
      "--------------------------------------------------\n",
      "(Train Set) Dream: I was being chased by a lion\n",
      "Predicted Interpretation: Dream: I was being chased by a lion. Freud would say this dream symbolizes fear, possibly connected to your feelings of guilt.\n",
      "--------------------------------------------------\n",
      "(Train Set) Dream: I was underwater in a deep ocean\n",
      "Predicted Interpretation: Dream: I was underwater in a deep ocean. Freud would say this dream symbolizes loneliness, possibly connected to your feelings of loss.\n",
      "--------------------------------------------------\n",
      "(Train Set) Dream: I found a hidden treasure chestI see baby open pandora box\n",
      "Predicted Interpretation: Dream: I found a hidden treasure chestI see baby open pandora box. Freud would say this dream symbolizes guilt, possibly connected to your feelings of guilt.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T20:02:58.084065Z",
     "start_time": "2025-02-05T20:02:58.029429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# Retrieve original dream texts **before tokenization**\n",
    "validation_dreams = val_df[\"text\"].tolist()  # Get dream texts from validation dataframe\n",
    "\n",
    "# Select 3 random dreams from the validation set\n",
    "sample_dreams = [\n",
    "    \"I was flying high above the clouds\",\n",
    "    \"I lost all my teeth\",\n",
    "    \"I was being chased by a lion\",\n",
    "    \"I was underwater in a deep ocean\",\n",
    "    \"I found a hidden treasure chest\"\n",
    "]\n",
    "\n",
    "# Generate interpretations for the selected dreams\n",
    "for dream in sample_dreams:\n",
    "    interpretation = predict_interpretation(dream)  # Function defined earlier\n",
    "    print(f\"Dream: {dream}\")\n",
    "    print(f\"Predicted Interpretation: {interpretation}\")\n",
    "    print(\"-\" * 50)\n"
   ],
   "id": "76e79ecddb6c8272",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_interpretation() missing 3 required positional arguments: 'tokenizer', 'device', and 'dream_input'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Generate interpretations for the selected dreams\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dream \u001B[38;5;129;01min\u001B[39;00m sample_dreams:\n\u001B[1;32m---> 17\u001B[0m     interpretation \u001B[38;5;241m=\u001B[39m \u001B[43mpredict_interpretation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdream\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Function defined earlier\u001B[39;00m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDream: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdream\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPredicted Interpretation: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minterpretation\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: predict_interpretation() missing 3 required positional arguments: 'tokenizer', 'device', and 'dream_input'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% Generate Dream Interpretation for Sample Inputs\n",
    "def generate_interpretation(model, tokenizer, device, dream_input, max_length=128):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    # Tokenize the input dream\n",
    "    input_text = f\"Dream: {dream_input}\\nInterpretation:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "\n",
    "    # Generate the interpretation (output text)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=256,  # Maximum length of the generated interpretation\n",
    "            num_beams=4,  # Beam search for more diverse outputs\n",
    "            no_repeat_ngram_size=2,  # Prevent repetition\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode the generated ids back to text\n",
    "    interpretation = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return interpretation\n",
    "\n",
    "# %% Example input dreams\n",
    "sample_dreams = [\n",
    "    \"I was flying high above the clouds\",\n",
    "    \"I lost all my teeth\",\n",
    "    \"I was being chased by a lion\",\n",
    "    \"I was underwater in a deep ocean\",\n",
    "    \"I found a hidden treasure chest\"\n",
    "]\n",
    "\n",
    "# %% Generate and display interpretations for sample dreams\n",
    "for dream in sample_dreams:\n",
    "    interpretation = generate_interpretation(model, tokenizer, device, dream)\n",
    "    print(f\"Dream: {dream}\")\n",
    "    print(f\"Interpretation: {interpretation}\")\n",
    "    print(\"=\"*50)"
   ],
   "id": "a6f9c529d32d9c21"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
