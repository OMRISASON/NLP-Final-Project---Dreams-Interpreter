{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9375785-880d-4158-b34e-785fa90f2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9efa5c0d-42d6-4e85-9654-87f197f83b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "df = pd.read_csv('dreams.csv')\n",
    "data = df[['Dream Symbol', 'Interpretation']].dropna()\n",
    "train_data, val_data = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ab58e4-4312-4585-bc74-f3cbe6f8a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and model setup\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2be00b-32b4-4bf0-8848-c2dc3ac0c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_data(examples):\n",
    "    inputs = [\"translate dream to interpretation: \" + ex for ex in examples['Dream Symbol']]\n",
    "    outputs = [ex for ex in examples['Interpretation']]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=True)\n",
    "\n",
    "    labels = tokenizer(outputs, max_length=128, truncation=True, padding=True).input_ids\n",
    "    model_inputs[\"Interpretation\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e3226b0-5bc3-471a-9106-32f4470bc46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(examples):\n",
    "    inputs = [\"translate dream to interpretation: \" + ex for ex in examples['Dream Symbol']]\n",
    "    outputs = [ex for ex in examples['Interpretation']]\n",
    "    \n",
    "    # Tokenize inputs and outputs\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(outputs, max_length=128, truncation=True, padding=True).input_ids\n",
    "    \n",
    "    # Replace padding token id with -100 to ignore it in loss calculation\n",
    "    labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f052980-d852-4767-805f-7047a84a37b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c20993e58444f8ba6b2e8900763e6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/721 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926712ce34f74b0a9ed14d970a5477d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/181 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert to Dataset object\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_train = Dataset.from_pandas(train_data)\n",
    "dataset_val = Dataset.from_pandas(val_data)\n",
    "\n",
    "tokenized_train = dataset_train.map(tokenize_data, batched=True)\n",
    "tokenized_val = dataset_val.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ec409b-614b-4886-890d-05496f783159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b907a710-9804-47db-beda-8437a8fbe327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "Transformers version: 4.46.1\n",
      "Accelerate version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Accelerate version:\", accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb894c87-5b1c-4a6f-876a-c1fe20875d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omri Sason\\AppData\\Local\\Temp\\ipykernel_8480\\1974856159.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29f3ab17-6395-4155-a9ba-b648f7c86a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='910' max='910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [910/910 1:55:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.254500</td>\n",
       "      <td>3.053847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.043600</td>\n",
       "      <td>2.773596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.869700</td>\n",
       "      <td>2.694330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.713700</td>\n",
       "      <td>2.645114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.722900</td>\n",
       "      <td>2.610678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.703100</td>\n",
       "      <td>2.589275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.651400</td>\n",
       "      <td>2.578536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.659500</td>\n",
       "      <td>2.569103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.589200</td>\n",
       "      <td>2.562772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.577000</td>\n",
       "      <td>2.560693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./dream_model\\\\tokenizer_config.json',\n",
       " './dream_model\\\\special_tokens_map.json',\n",
       " './dream_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./dream_model\")\n",
    "tokenizer.save_pretrained(\"./dream_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "787ecdf7-4948-48ec-85e9-fa874d26f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example\n",
    "def generate_interpretation(dream_text):\n",
    "    input_ids = tokenizer(\"interperate dream: \" + dream_text, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_length=100, temperature=0.7, top_k=50, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8171b127-5f29-4477-bdd8-85f1c27b07db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dream interpretation: dream: I dreamed that i am a big dog\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "dream = \"I dreamed that i am a big dog\"\n",
    "interpretation = generate_interpretation(dream)\n",
    "print(\"Dream interpretation:\", interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58df5640-ac0e-424b-8691-b2301f81dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dream interpretation: 'I dreamed that i am a big dog'. Explain the psychological meaning behind the dream, considering Freudian concepts like wish fulfillment, repressed desires, or unconscious thoughts. Explain the symbolism of the elements in the dream and how they relate to the dreamer's psyche.\n"
     ]
    }
   ],
   "source": [
    "def generate_interpretation(dream_text):\n",
    "    # Making the prompt even more specific and guiding the model to provide a comprehensive Freudian analysis\n",
    "    prompt = (f\"Provide a detailed Freudian interpretation of the following dream: '{dream_text}'. \"\n",
    "              \"Explain the psychological meaning behind the dream, considering Freudian concepts like \"\n",
    "              \"wish fulfillment, repressed desires, or unconscious thoughts. Describe the symbolism of the elements \"\n",
    "              \"in the dream and how they relate to the dreamer's psyche.\")\n",
    "    # Generate the interpretation\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_length=250, temperature=0.8, top_k=50, num_return_sequences=1)\n",
    "    \n",
    "    # Decoding the output and cleaning it up\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up the output to focus on just the interpretation\n",
    "    interpretation = decoded_output.split(\":\")[-1].strip()  # Ensuring we get the actual interpretation\n",
    "    \n",
    "    return interpretation\n",
    "\n",
    "# Example usage\n",
    "dream = \"I dreamed that i am a big dog\"\n",
    "interpretation = generate_interpretation(dream)\n",
    "print(\"Dream interpretation:\", interpretation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b11678ad-195d-42a9-8651-b7e516d8cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated CSV saved to: generated_interpretations.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file containing dreams and interpretations\n",
    "file_path = 'dreams_and_interpretations_Freud.xlsx'  # Replace with your file path if needed\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Extract the necessary columns\n",
    "dreams = data['Dream']\n",
    "original_interpretations = data['Interpretation']\n",
    "\n",
    "# Apply the custom function to each dream\n",
    "generated_interpretations = dreams.apply(generate_interpretation)\n",
    "\n",
    "# Combine the original and generated interpretations\n",
    "output_df = pd.DataFrame({\n",
    "    'Original_Interpretation': original_interpretations,\n",
    "    'Generated_Interpretation': generated_interpretations\n",
    "})\n",
    "\n",
    "# Save the output to a new CSV file\n",
    "output_file_path = 'generated_interpretations.csv'\n",
    "output_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Generated CSV saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55bf1085-ddd1-41b2-a482-b3cdbe0a6d82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_bleu\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rouge\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2LMHeadModel, GPT2TokenizerFast\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m score \u001b[38;5;28;01mas\u001b[39;00m bert_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rouge'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from bert_score import score as bert_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3db12-e901-4614-8c62-433bef1d03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the function to calculate perplexity using GPT-2\n",
    "def calculate_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss.item()\n",
    "    return 2 ** loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4da15ba-efb9-4c20-9372-ab4af52a21a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_excel('generated_interpretations.xls')\n",
    "\n",
    "# Extract columns\n",
    "original_interpretations = data['Original_Interpretation']\n",
    "generated_interpretations = data['Generated_Interpretation']\n",
    "\n",
    "# Initialize models and metrics\n",
    "rouge = Rouge()\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Compute metrics for each row\n",
    "metrics_data = []\n",
    "for original, generated in zip(original_interpretations, generated_interpretations):\n",
    "    bleu = sentence_bleu([original.split()], generated.split())\n",
    "    rouge_scores = rouge.get_scores(generated, original)[0]\n",
    "    perplexity = calculate_perplexity(generated)\n",
    "    bert_p, bert_r, bert_f = bert_score([generated], [original], lang='en')\n",
    "\n",
    "    metrics_data.append({\n",
    "        'Original': original,\n",
    "        'Generated': generated,\n",
    "        'BLEU': bleu,\n",
    "        'ROUGE_L': rouge_scores['rouge-l']['f'],\n",
    "        'Perplexity': perplexity,\n",
    "        'BERTScore': bert_f.mean().item()\n",
    "    })\n",
    "\n",
    "# Save results to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('dreams_interpretation_metrics.csv', index=False)\n",
    "\n",
    "print(\"Metrics saved to 'dreams_interpretation_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045adfef-7fef-4877-b7a6-a227b8f8763b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
